{"history":"What%20is%20this%20article%20about%3F%0A%0AAzure%20Well-Architected%20Framework%20perspective%20on%20Azure%20OpenAI%0AArticle%0A02%2F26%2F2024%0A3%20contributors%0AIn%20this%20article%0AReliability%0ASecurity%0ACost%20Optimization%0AOperational%20Excellence%0AShow%202%20more%0AThe%20Azure%20OpenAI%20Service%20provides%20REST%20API%20access%20to%20OpenAI's%20large%20language%20models%20(LLMs)%2C%20adding%20Azure%20networking%20and%20security%20capabilities.%20This%20article%20provides%20architectural%20recommendations%20for%20making%20informed%20decisions%20when%20using%20Azure%20OpenAI%20as%20part%20of%20your%20workload's%20architecture.%20The%20guidance%20is%20based%20on%20the%20Azure%20Well-Architected%20Framework%20pillars.%0A%0A%20Important%0A%0AHow%20to%20use%20this%20guide%0A%0AEach%20section%20has%20a%20design%20checklist%20that%20presents%20architectural%20areas%20of%20concern%20along%20with%20design%20strategies%20localized%20to%20the%20technology%20scope.%0A%0AAlso%20included%20are%20recommendations%20on%20the%20technology%20capabilities%20that%20can%20help%20materialize%20those%20strategies.%20The%20recommendations%20don't%20represent%20an%20exhaustive%20list%20of%20all%20configurations%20available%20for%20Azure%20OpenAI.%20Instead%2C%20they%20list%20the%20key%20recommendations%20mapped%20to%20the%20design%20perspectives.%20Use%20the%20recommendations%20to%20build%20your%20proof-of-concept%20or%20optimize%20your%20existing%20environments.%0A%0AThe%20following%20foundational%20architecture%20demonstrates%20many%20of%20the%20key%20recommendations%3A%20Baseline%20OpenAI%20end-to-end%20chat%20reference%20architecture.%0A%0ATechnology%20scope%0AThis%20review%20focuses%20solely%20on%20the%20Azure%20OpenAI%20service.%0A%0AReliability%0AThe%20purpose%20of%20the%20Reliability%20pillar%20is%20to%20provide%20continued%20functionality%20by%20building%20enough%20resilience%20and%20the%20ability%20to%20recover%20fast%20from%20failures.%0A%0AThe%20Reliability%20design%20principles%20provide%20a%20high-level%20design%20strategy%20applied%20for%20individual%20components%2C%20system%20flows%2C%20and%20the%20system%20as%20a%20whole.%0A%0ADesign%20checklist%0AStart%20your%20design%20strategy%20based%20on%20the%20design%20review%20checklist%20for%20Reliability%20and%20determine%20its%20relevance%20to%20your%20business%20requirements.%20Extend%20the%20strategy%20to%20include%20more%20approaches%20as%20needed.%0A%0AResilience%20Choose%20the%20right%20deployment%20options%20of%20either%20pay-as-you%20go%20or%20provisioned%20throughput%20based%20on%20your%20use%20case.%20Because%20reserved%20capacity%20increases%20resiliency%2C%20choose%20provisioned%20throughput%20for%20production%20solutions.%20Pay-as-you-go%20is%20ideal%20for%20dev%2Ftest%20environments.%0A%0ARedundancy%20Add%20the%20appropriate%20gateway(s)%20in%20front%20of%20your%20Azure%20OpenAI%20deployments%20that%20have%20the%20capability%20to%20withstand%20transient%20failures%20such%20as%20throttling%20and%20can%20route%20to%20multiple%20Azure%20OpenAI%20instances.%20Consider%20routing%20to%20instances%20in%20different%20regions%20to%20build%20regional%20redundancy.%0A%0AResilience%20If%20you're%20using%20provisioned%20throughput%2C%20consider%20deploying%20a%20pay-as-you-go%20instance%2C%20in%20addition%2C%20to%20handle%20overflow.%20Through%20your%20gateway%2C%20route%20calls%20to%20the%20pay-as-you-go%20instance%20when%20your%20provisioned%20throughput%20model%20is%20being%20throttled%2C%20or%2C%20through%20monitoring%2C%20when%20you%20predict%20you%20will%20be%20throttled.%0A%0AResilience%20Monitor%20capacity%20usage%20to%20ensure%20you%20aren't%20exceeding%20throughput%20limits.%20Regularly%20reviewing%20capacity%20usage%20also%20allows%20for%20more%20accurate%20forecasting%20and%20can%20help%20prevent%20service%20interruptions%20due%20to%20capacity%20constraints.%0A%0AResilience%20Follow%20the%20guidance%20in%20customize%20a%20model%20with%20fine-tuning%20for%20large%20data%20files%20by%20importing%20the%20data%20from%20Azure%20Blob%20store.%20The%20guidance%20states%20that%20large%20files%20can%20become%20unstable%20when%20uploaded%20through%20multipart%20forms%20because%20the%20requests%20are%20atomic%20and%20can't%20be%20retried%20or%20resumed.%20Consider%20files%20100MB%20or%20larger%20as%20large%20files.%0A%0ARecovery%20Ensure%20you%20have%20a%20recovery%20strategy%20defined%20that%20includes%20a%20recovery%20plan%20for%20models%20that%20have%20been%20fine-tuned%20and%20for%20training%20data%20uploaded%20to%20the%20Azure%20OpenAI%20service.%20Azure%20OpenAI%20doesn't%20have%20automatic%20failover.%20Therefore%2C%20you%20must%20design%20a%20strategy%20that%20encompasses%20the%20Azure%20OpenAI%20service%20and%20all%20of%20its%20dependencies%2C%20such%20as%20storage%20that%20contains%20training%20data.%0A%0ARecommendations%0ARecommendation%09Benefit%0AMonitor%20rate%20limits%20for%20pay-as-you-go%20If%20you're%20using%20pay-as-you-go%2C%20follow%20the%20guidance%20to%20manage%20rate%20limits%20for%20your%20model%20deployments%20and%20monitor%20usage%20of%20Tokens%20per%20Minute%20(TPM)%20and%20Requests%20per%20Minute%20(RPM).%09This%20important%20throughput%20information%20provides%20you%20with%20the%20information%20required%20to%20ensure%20you%20assign%20enough%20TPM%20from%20your%20quota%20to%20meet%20the%20demand%20for%20your%20deployments.%0AAssigning%20enough%20quota%20prevents%20calls%20to%20your%20deployed%20models%20from%20being%20throttled.%0AMonitor%20provisioned-managed%20utilization%20for%20provisioned%20throughput%20If%20you're%20using%20provisioned%20throughput%20payment%20model%2C%20monitor%20provision-managed%20utilization.%09It's%20important%20to%20monitor%20provision-managed%20utilization%20to%20ensure%20it%20doesn't%20exceed%20100%25%20to%20prevent%20calls%20to%20your%20deployed%20models%20from%20being%20throttled.%0AEnable%20dynamic%20quota%20If%20your%20workload%20budget%20supports%20it%2C%20perform%20overprovisoning%20by%20enabling%20dynamic%20quota%20on%20model%20deployments.%09Dynamic%20quota%20allows%20your%20deployment%20to%20consume%20more%20capacity%20than%20your%20quota%20normally%20would%2C%20as%20long%20as%20there%20is%20available%20capacity%20from%20Azure's%20perspective.%20While%20it%20is%20not%20guaranteed%2C%20the%20chance%20of%20additional%20quota%20could%20prevent%20undesired%20throttling.%0ATune%20content%20filters%20Tune%20content%20filters%20to%20minimize%20false%20positives%20from%20overly%20aggressive%20filters.%09Content%20filters%20block%20prompts%20or%20completions%20based%20on%20an%20opaque%20risk%20analysis.%20Ensure%20content%20filters%20are%20tuned%20to%20allow%20expected%20usage%20for%20your%20workload.%0AAzure%20Policy%20and%20Azure%20Advisor%0AThere%20are%20no%20Azure%20Advisor%20best%20practice%20recommendations%20or%20built-in%20Azure%20Policy%20definitions%20for%20reliability%20for%20Azure%20OpenAI.%0A%0ASecurity%0AThe%20purpose%20of%20the%20Security%20pillar%20is%20to%20provide%20confidentiality%2C%20integrity%2C%20and%20availability%20guarantees%20to%20the%20workload.%0A%0AThe%20Security%20design%20principles%20provide%20a%20high-level%20design%20strategy%20for%20achieving%20those%20goals%20by%20applying%20approaches%20to%20the%20technical%20design%20around%20Azure%20OpenAI.%0A%0ADesign%20checklist%0AStart%20your%20design%20strategy%20based%20on%20the%20design%20review%20checklist%20for%20Security%20and%20identify%20vulnerabilities%20and%20controls%20to%20improve%20the%20security%20posture.%20Then%20review%20the%20Azure%20security%20baseline%20for%20Azure%20OpenAI.%20Finally%2C%20extend%20the%20strategy%20to%20include%20more%20approaches%20as%20needed.%0A%0AProtect%20confidentiality%20If%20you're%20uploading%20training%20data%20to%20the%20Azure%20OpenAI%20service%2C%20use%20customer%20managed%20keys%20for%20data%20encryption%20and%20implement%20a%20key%20rotation%20strategy%20and%20follow%20guidance%20on%20deleting%20training%2C%20validation%20and%20training%20results%20data.%20If%20you're%20using%20an%20external%20data%20store%20for%20training%20data%2C%20follow%20security%20best%20practices%20for%20that%20store.%20For%20example%2C%20for%20Azure%20Blob%20Storage%2C%20use%20customer%20managed%20keys%20for%20encryption%20and%20implement%20a%20key%20rotation%20strategy%2C%20use%20managed%20identity-based%20access%2C%20implement%20a%20network%20perimeter%20using%20private%20endpoints%20and%20enable%20access%20logs.%0A%0AProtect%20confidentiality%20Guard%20against%20data%20exfiltration%20by%20limiting%20the%20outbound%20URLs%20the%20Azure%20OpenAI%20services%20resources%20are%20allowed%20to%20access.%0A%0AProtect%20integrity%20Implement%20access%20controls%20to%20authenticate%20and%20authorize%20user%20access%20to%20the%20system%20using%20the%20least%20privilege%20principle%20and%20using%20individual%20identities%20instead%20of%20keys.%0A%0AProtect%20integrity%20Follow%20the%20guidance%20in%20jailbreak%20risk%20detection%20to%20safeguard%20your%20LLM%20deployments%20against%20prompt%20injection%20attacks.%0A%0AProtect%20availability%20Use%20security%20controls%20to%20prevent%20attacks%20that%20will%20exhaust%20model%20usage%20quotas.%20These%20controls%20can%20include%20network%20isolating%20the%20service.%20If%20the%20service%20must%20be%20internet%20accessible%2C%20consider%20using%20a%20gateway%20to%20make%20routing%20or%20throttling%20decisions%20that%20block%20suspected%20abuse.%0A%0ARecommendations%0ARecommendation%09Benefit%0ASecure%20keys%20If%20your%20architecture%20requires%20Azure%20OpenAI%20key-based%20authentication%2C%20store%20those%20keys%20in%20Azure%20Key%20Vault%2C%20not%20in%20application%20code.%09Separating%20secrets%20from%20code%2C%20such%20as%20storing%20them%20in%20Azure%20Key%20Vault%2C%20reduces%20the%20chances%20of%20leaking%20secrets%20and%20allows%20the%20central%20management%20of%20secrets%2C%20easing%20responsibilities%20such%20as%20rotating%20keys.%0ARestrict%20access%20Disable%20public%20access%20to%20Azure%20OpenAI%2C%20unless%20your%20workload%20requires%20it.%20Create%20private%20endpoints%20if%20you%20are%20connecting%20from%20consumers%20in%20an%20Azure%20Virtual%20Network.%09Controlling%20access%20to%20Azure%20OpenAI%20helps%20prevent%20attacks%20from%20unauthorized%20users.%20Using%20private%20endpoints%20ensures%20network%20traffic%20remains%20private%20between%20the%20application%20and%20the%20platform.%0AMicrosoft%20Entra%20ID%20Use%20Microsoft%20Entra%20ID%20for%20authentication%20and%20to%20authorize%20access%20to%20OpenAI%20using%20role-based%20access%20control%20(RBAC).%20Follow%20the%20guidance%20in%20Disable%20local%20authentication%20in%20Azure%20AI%20Services%20and%20set%20disableLocalAuth%20to%20true.%20Identities%20performing%20completions%20or%20image%20generation%20should%20be%20granted%20Cognitive%20Services%20OpenAI%20User%2C%20while%20model%20automation%20pipelines%20and%20ad-hoc%20data%20science%20access%20should%20be%20granted%20a%20role%20like%20Cognitive%20Services%20OpenAI%20Contributor.%09Using%20Microsoft%20Entra%20ID%20centralizes%20the%20identity%20management%20component%20and%20eliminates%20the%20use%20of%20API%20keys.%20Using%20RBAC%20with%20Entra%20ID%20ensures%20users%20or%20groups%20have%20exactly%20the%20permissions%20they%20need%20to%20do%20their%20job.%20This%20kind%20of%20fine-grained%20access%20control%20isn't%20possible%20with%20Azure%20OpenAI%20API%20keys.%0AUse%20customer-managed%20keys%20Follow%20the%20guidance%20in%20Azure%20OpenAI%20Service%20encryption%20of%20data%20at%20rest%20to%20use%20customer%20managed%20keys%20for%20fine-tuned%20models%20and%20training%20data%20uploaded%20to%20the%20OpenAI%20service.%09Using%20customer%20managed%20keys%20gives%20you%20greater%20flexibility%20to%20create%2C%20rotate%2C%20disable%2C%20and%20revoke%20access%20controls.%0AProtect%20against%20jailbreak%20Use%20Azure%20Content%20Safety%20Studio%20to%20detect%20jailbreak%20risks.%09Detecting%20jailbreak%20attempts%20allows%20you%20to%20identify%20and%20block%20prompts%20intending%20to%20bypass%20the%20safety%20mechanisms%20of%20your%20Azure%20OpenAI%20deployments.%0AAzure%20Advisor%20and%20Azure%20Policy%0AThe%20following%20are%20examples%20of%20built-in%20Azure%20Policy%20definitions%20for%20Azure%20OpenAI%20security%3A%0A%0ADisable%20key%20access%0ARestrict%20network%20access%0ADisable%20public%20network%20access%0AUse%20private%20link%0AEnable%20data%20encryption%20with%20customer-managed%20key%0AThe%20above%20Azure%20Policy%20definitions%20are%20also%20Azure%20Advisor%20security%20best%20practice%20recommendations%20for%20Azure%20OpenAI.%0A%0ACost%20Optimization%0ACost%20Optimization%20focuses%20on%20detecting%20spend%20patterns%2C%20prioritizing%20investments%20in%20critical%20areas%2C%20and%20optimizing%20in%20others%20to%20meet%20the%20organization's%20budget%20while%20meeting%20business%20requirements.%0A%0ARead%20the%20Cost%20Optimization%20design%20principles%20to%20understand%20the%20approaches%20for%20achieving%20those%20goals%20and%20understand%20the%20tradeoffs%20necessary%20in%20technical%20design%20choices%20related%20to%20Azure%20OpenAI.%0A%0ADesign%20checklist%0AStart%20your%20design%20strategy%20based%20on%20the%20design%20review%20checklist%20for%20Cost%20Optimization%20for%20investments%20and%20fine%20tune%20the%20design%20so%20that%20the%20workload%20is%20aligned%20with%20the%20budget%20allocated%20for%20the%20workload.%20Your%20design%20should%20use%20the%20right%20Azure%20capabilities%2C%20monitor%20investments%2C%20and%20find%20opportunities%20to%20optimize%20over%20time.%0A%0ACost%20management%20Develop%20your%20cost%20model%2C%20considering%20prompt%20sizes.%20Understanding%20prompt%20input%20and%20response%20sizes%20and%20how%20text%20translates%20into%20tokens%20will%20allow%20you%20to%20create%20a%20viable%20cost%20model.%0A%0AUsage%20optimization%20Start%20with%20pay-as-you-go%20pricing%20for%20Azure%20OpenAI%20until%20your%20token%20utilization%20is%20predictable.%0A%0ARate%20optimization%20When%20your%20token%20utilization%20is%20sufficiently%20high%20and%20predictable%20over%20a%20period%20of%20time%2C%20use%20the%20provisioned%20throughput%20pricing%20model%20for%20better%20cost%20optimization.%0A%0AUsage%20optimization%20Take%20model%20pricing%20into%20account%20along%20with%20model%20capabilities%20when%20choosing%20models.%20Start%20with%20less%20costly%20models%20for%20less%20complex%20tasks%20like%20text%20generation%20or%20completion%20tasks.%20For%20more%20complex%20tasks%20like%20language%20translation%2C%20or%20content%20understanding%2C%20consider%20more%20advanced%20models.%20Consider%20different%20model%20capabilities%20and%20maximum%20token%20usage%20limits%20while%20picking%20the%20appropriate%20model%20aligning%20with%20the%20use%20cases%20like%20text%20embedding%2C%20image%20generation%2C%20transcription%20scenarios%2C%20etc.%20By%20carefully%20selecting%20the%20model%20that%20best%20fits%20your%20needs%2C%20you%20can%20optimize%20costs%20while%20still%20achieving%20the%20desired%20performance%20for%20your%20application.%0A%0AUsage%20optimization%20Use%20the%20token-limiting%20constraints%20offered%20by%20the%20API%20calls%2C%20such%20as%20max_tokens%20and%20n%20which%20indicates%20the%20number%20of%20completions%20to%20generate.%0A%0AUsage%20optimization%20Maximize%20the%20price%20break%20points%20of%20the%20OpenAI%20service%2C%20like%20fine-tuning%2C%20and%20model%20break%20points%2C%20like%20image%20generation.%20Fine-tuning%20is%20charged%20per-hour.%20To%20be%20the%20most%20efficient%2C%20you'll%20want%20to%20utilize%20as%20much%20of%20that%20time%20available%20per%20hour%20to%20improve%20the%20fine-tuning%20results%20while%20avoiding%20just%20slipping%20into%20the%20next%20billing%20period.%20Likewise%2C%20the%20cost%20for%20100%20images%20from%20image%20generation%20is%20the%20same%20as%20the%20cost%20for%201%20image.%20Maximize%20the%20price%20break%20points%20to%20your%20advantage.%0A%0AUsage%20optimization%20Remove%20unused%20fine-tuned%20models%20when%20they%20are%20no%20longer%20being%20consumed%2C%20as%20they%20incur%20a%20ongoing%20hosting%20fee.%0A%0AAdjust%20usage%20Optimize%20prompt%20input%20and%20response%20length.%20Longer%20prompts%20consume%20more%20tokens%2C%20raising%20the%20cost.%20However%2C%20prompts%20that%20are%20missing%20sufficient%20context%20will%20not%20help%20the%20models%20yield%20good%20results.%20Create%20concise%20prompts%20that%20provide%20enough%20context%20to%20allow%20the%20model%20to%20generate%20a%20useful%20response.%20Likewise%2C%20ensure%20you%20optimize%20the%20limit%20of%20the%20response%20length.%0A%0ACost%20efficiency%20Batch%20requests%2C%20where%20possible%2C%20to%20minimize%20the%20per-call%20overhead%20which%20can%20reduce%20overall%20costs.%20Ensure%20you%20optimize%20batch%20size.%0A%0ACost%20efficiency%20Models%20have%20different%20fine-tuning%20costs%20which%20should%20be%20considered%20if%20required%20in%20your%20solution.%0A%0AMonitor%20and%20optimize%20Set%20up%20a%20cost%20tracking%20system%20that%20monitors%20model%20usage%20and%20use%20that%20information%20to%20help%20inform%20model%20choices%20and%20prompt%20sizes.%0A%0ARecommendations%0ARecommendation%09Benefit%0ADesign%20client%20code%20to%20set%20limits%20Your%20custom%20clients%20should%20use%20the%20limit%20features%20of%20the%20Azure%20OpenAI%20completions%20API%2C%20such%20as%20maximum%20limit%20on%20the%20number%20of%20tokens%20per%20model%20(max_tokens)%20or%20number%20of%20completions%20to%20generation%20(n)%20to%20ensure%20the%20server%20doesn't%20produce%20more%20than%20the%20client%20needs.%09Using%20the%20API%20features%20to%20restrict%20usage%20aligns%20the%20consumption%20of%20the%20service%20to%20the%20needs%20of%20the%20client.%20This%20saves%20money%20by%20ensuring%20the%20model%20doesn't%20generate%20an%20overly%20long%20response%20that%20consumes%20more%20tokens%20than%20necessary.%0AMonitor%20pay-as-you-go%20usage%20If%20using%20pay-as-you-go%2C%20monitor%20usage%20of%20Tokens%20per%20Minute%20(TPM)%20and%20Requests%20per%20Minute%20(RPM).%20Use%20that%20information%20to%20inform%20architectural%20design%20decisions%20such%20as%20what%20models%20to%20use%2C%20as%20well%20as%20optimize%20prompt%20sizes.%09Continuously%20monitoring%20TPM%20and%20RPM%20provides%20you%20with%20the%20relevant%20metrics%20to%20optimize%20the%20cost%20of%20OpenAI%20models.%20You%20can%20couple%20this%20monitoring%20with%20model%20features%20and%20model%20pricing%20to%20optimize%20model%20usage.%20You%20can%20also%20use%20this%20monitoring%20to%20optimize%20prompt%20sizes.%0AMonitor%20provisioned%20throughput%20usage%20If%20using%20provisioned%20throughput%2C%20monitor%20provision-managed%20utilization%20to%20ensure%20you're%20not%20underutilizing%20the%20provisioned%20throughput%20you%20purchased.%09Continuously%20monitoring%20provision-managed%20utilization%20provides%20you%20with%20the%20information%20you%20need%20to%20understand%20if%20you're%20underutilizing%20your%20provisioned%20throughput.%0ACost%20management%20Follow%20the%20guidance%20on%20using%20cost%20management%20features%20with%20OpenAI%20to%20monitor%20costs%2C%20set%20budgets%20to%20manage%20costs%2C%20and%20create%20alerts%20to%20notify%20stakeholders%20of%20risks%20or%20anomalies.%09Cost%20monitoring%2C%20setting%20budgets%2C%20and%20setting%20alerts%20provides%20governance%20with%20the%20appropriate%20accountability%20processes.%0AAzure%20Policy%20and%20Azure%20Advisor%0AThere%20are%20no%20Azure%20Advisor%20best%20practice%20recommendations%20or%20built-in%20Azure%20Policy%20definitions%20for%20cost%20optimization%20for%20Azure%20OpenAI.%0A%0AOperational%20Excellence%0AOperational%20Excellence%20primarily%20focuses%20on%20procedures%20for%20development%20practices%2C%20observability%2C%20and%20release%20management.%0A%0AThe%20Operational%20Excellence%20design%20principles%20provide%20a%20high-level%20design%20strategy%20for%20achieving%20those%20goals%20towards%20the%20operational%20requirements%20of%20the%20workload.%0A%0ADesign%20checklist%0AStart%20your%20design%20strategy%20based%20on%20the%20design%20review%20checklist%20for%20Operational%20Excellence%20for%20defining%20processes%20for%20observability%2C%20testing%2C%20and%20deployment%20related%20to%20Azure%20OpenAI.%0A%0ADevOps%20culture%20Ensure%20there%20are%20Azure%20OpenAI%20Service%20instances%20deployed%20across%20your%20various%20environments%2C%20such%20as%20development%2C%20test%2C%20and%20production%20and%20ensure%20you%20have%20enviornments%20to%20support%20continuous%20learning%20and%20an%20experimentation%20mindset%20throughout%20the%20development%20cycle.%0A%0AObservability%20Monitor%2C%20aggregate%20and%20visualize%20the%20appropriate%20metrics.%0A%0AObservability%20If%20the%20Azure%20OpenAI%20diagnostics%20aren't%20sufficient%20for%20you%2C%20consider%20using%20a%20gateway%20such%20as%20Azure%20API%20Management%20in%20front%20of%20Azure%20OpenAI%20to%20log%20both%20incoming%20prompts%20and%20outgoing%20responses%2C%20where%20permitted.%20This%20information%20can%20be%20used%20to%20help%20understand%20the%20effectiveness%20of%20the%20model%20for%20incoming%20prompts.%0A%0ADeploy%20with%20confidence%20Use%20Infrastructure%20as%20code%20to%20deploy%20the%20Azure%20OpenAI%20Service%2C%20model%20deployments%2C%20and%20other%20infrastructure%20required%20for%20model%20fine-tuning.%0A%0ADeploy%20with%20confidence%20Follow%20LLMOps%20practices%20to%20operationalize%20the%20management%20of%20your%20Azure%20OpenAI%20Service%20large%20language%20models%2C%20including%20deployment%2C%20fine-tuning%2C%20and%20prompt%20engineering.%0A%0AAutomate%20for%20efficiency%20If%20you're%20using%20key-based%20authentication%2C%20implement%20an%20automated%20key%20rotation%20strategy.%0A%0ARecommendations%0ARecommendation%09Benefit%0AEnable%20and%20configure%20Azure%20Diagnostics%20Enable%20and%20configure%20Azure%20Diagnostics%20for%20the%20Azure%20OpenAI%20Service.%09Enabling%20Azure%20Diagnostics%20allows%20you%20to%20collect%20and%20analyze%20metrics%20and%20logs%2C%20helping%20you%20monitor%20the%20availability%2C%20performance%2C%20and%20operation%20of%20the%20Azure%20OpenAI%20Service.%0AAzure%20Policy%20and%20Azure%20Advisor%0AThere%20are%20no%20Azure%20Advisor%20best%20practice%20recommendations%20or%20built-in%20Azure%20Policy%20definitions%20for%20operational%20excellence%20for%20Azure%20OpenAI.%0A%0APerformance%20Efficiency%0APerformance%20Efficiency%20is%20about%20maintaining%20user%20experience%20even%20when%20there's%20an%20increase%20in%20load%20by%20managing%20capacity.%20The%20strategy%20includes%20scaling%20resources%2C%20identifying%20and%20optimizing%20potential%20bottlenecks%2C%20and%20optimizing%20for%20peak%20performance.%0A%0AThe%20Performance%20Efficiency%20design%20principles%20provide%20a%20high-level%20design%20strategy%20for%20achieving%20those%20capacity%20goals%20against%20the%20expected%20usage.%0A%0ADesign%20checklist%0AStart%20your%20design%20strategy%20based%20on%20the%20design%20review%20checklist%20for%20Performance%20Efficiency%20for%20defining%20a%20baseline%20based%20on%20key%20performance%20indicators%20for%20Azure%20OpenAI%20workloads.%0A%0ACapacity%20Estimate%20elasticity%20demands%20of%20consumers.%20Determine%20traffic%20that%20will%20be%20high%20priority%20and%20require%20synchronous%20responses%20and%20traffic%20that%20is%20low%20priority%20and%20can%20be%20asynchronous%20and%20batched.%0A%0ACapacity%20Benchmark%20token%20consumption%20requirements%20based%20on%20estimated%20demands%20from%20consumers.%20Consider%20using%20the%20Azure%20OpenAI%20benchmarking%20tool%20to%20help%20you%20validate%20the%20throughput%20if%20you're%20using%20Provisioned%20Throughput%20Unit%20deployments.%0A%0ACapacity%20Use%20provisioned%20throughput%20for%20production%20workloads.%20Provisioned%20throughput%20offers%20dedicated%20memory%20and%20compute%2C%20reserved%20capacity%2C%20and%20consistent%20max%20latency%20for%20given%20model-version.%20The%20pay-as-you-go%20offering%20can%20suffer%20from%20noisy%20neighbor%20problems%2C%20such%20as%20increased%20latency%20and%20throttling%2C%20in%20regions%20under%20heavy%20utilization.%20Further%2C%20the%20pay-as-you-go%20doesn't%20offer%20guaranteed%20capacity.%0A%0ACapacity%20Add%20the%20appropriate%20gateway(s)%20in%20front%20of%20your%20OpenAI%20deployments%20that%20can%20route%20to%20multiple%20instances%20in%20the%20same%20or%20different%20regions.%0A%0ACapacity%20Allocate%20Provisioned%20Throughput%20Units%20(PTUs)%20to%20cover%20your%20predicted%20usage%2C%20and%20complement%20it%20with%20a%20token-per-minute%20(TPM)%20deployment%20to%20handle%20elasticity%20above%20that%20limit.%20This%20combines%20base%20throughput%20with%20elastic%20throughput%20for%20efficiency.%20Like%20other%20considerations%2C%20this%20requires%20a%20custom%20gateway%20implementation%20to%20route%20requests%20to%20the%20TPM%20deployment%20when%20the%20PTU%20limits%20are%20reached.%0A%0ACapacity%20Send%20high-priority%20requests%20synchronously.%20Queue%20low%20priority%20requests%20and%20send%20them%20through%20in%20batches%20when%20demand%20is%20low.%0A%0ACapacity%20Select%20a%20model%20that%20aligns%20with%20your%20performance%20requirements%2C%20considering%20the%20trade-off%20between%20speed%20and%20output%20complexity.%20Model%20performance%20can%20vary%20significantly%20based%20on%20the%20type%20of%20model%20chosen.%20Models%20designed%20for%20speed%2C%20offer%20faster%20response%20times%2C%20which%20can%20be%20beneficial%20for%20applications%20requiring%20quick%20interactions.%20On%20the%20other%20hand%2C%20more%20sophisticated%20models%20may%20deliver%20higher%20quality%20outputs%20at%20the%20expense%20of%20increased%20response%20time.%0A%0AAchieve%20performance%20For%20applications%20like%20chat%20bots%20or%20conversational%20interfaces%2C%20consider%20implementing%20streaming.%20Streaming%20can%20enhance%20the%20perceived%20performance%20of%20Azure%20OpenAI%20applications%20by%20delivering%20responses%20to%20users%20in%20an%20incremental%20manner%2C%20improving%20the%20user%20experience.%0A%0AAchieve%20performance%20Follow%20the%20guidance%20in%20When%20to%20use%20Azure%20OpenAI%20fine-tuning%20before%20committing%20to%20fine-tuning.%20While%20there%20are%20good%20use%20cases%20for%20fine-tuning%2C%20such%20as%20when%20the%20information%20needed%20to%20steer%20the%20model%20is%20too%20long%20or%20complex%20to%20fit%20into%20the%20prompt%2C%20you%20should%20make%20sure%20that%20prompt%20engineering%20and%20RAG%20based%20approaches%20will%20not%20work%20or%20will%20be%20demonstrably%20more%20expensive.%0A%0AAchieve%20performance%20Consider%20using%20dedicated%20model%20deployments%20per%20consumer%20group%20to%20provide%20per-model%20usage%20isolation%20which%20can%20help%20prevent%20noisy%20neighbors%20between%20your%20consumer%20groups.%0A%0ARecommendations%0AThere%20are%20no%20recommended%20configurations%20for%20Performance%20Efficiency%20for%20the%20Azure%20OpenAI%20service.%0A%0AAzure%20Policy%20and%20Azure%20Advisor%0AThere%20are%20no%20Azure%20Advisor%20best%20practice%20recommendations%20or%20built-in%20Azure%20Policy%20definitions%20for%20performance%20efficiency%20for%20Azure%20OpenAI.%0A%0ANext%20steps%0AConsider%20these%20articles%20as%20resources%20that%20demonstrate%20the%20recommendations%20highlighted%20in%20this%20article.%0A%0AUse%20this%20reference%20architecture%20as%20an%20example%20of%20how%20these%20recommendations%20can%20be%20applied%20to%20a%20workload%20such%20as%20the%20Baseline%20OpenAI%20end-to-end%20chat%20reference%20architecture.%0ABuild%20implementation%20expertise%20using%20product%20documentation%3A%20Azure%20Machine%20Learning%0A\n%20This%20article%20provides%20architectural%20recommendations%20for%20making%20informed%20decisions%20when%20using%20Azure%20OpenAI%20as%20part%20of%20your%20workload's%20architecture%2C%20based%20on%20the%20Azure%20Well-Architected%20Framework%20pillars%20of%20Reliability%2C%20Security%2C%20Cost%20Optimization%2C%20Operational%20Excellence%2C%20and%20Performance%20Efficiency.%20The%20article%20includes%20design%20checklists%20and%20recommendations%20for%20each%20pillar%20to%20help%20build%20a%20resilient%2C%20secure%2C%20cost-effective%2C%20operationally%20excellent%2C%20and%20performant%20solution%20using%20Azure%20OpenAI.%20The%20technology%20scope%20of%20the%20review%20is%20solely%20on%20the%20Azure%20OpenAI%20service.","model":"mistral:7b-instruct-v0.2-q4_K_M","systemText":"You are a article summarizer","selectedOption":"notepad"}